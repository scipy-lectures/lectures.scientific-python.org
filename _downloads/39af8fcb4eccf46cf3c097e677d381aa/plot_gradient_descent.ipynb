{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gradient descent\n\nAn example demoing gradient descent by creating figures that trace the\nevolution of the optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy as sp\n\nimport sys, os\n\nsys.path.append(os.path.abspath(\"helper\"))\nfrom cost_functions import (\n    mk_quad,\n    mk_gauss,\n    rosenbrock,\n    rosenbrock_prime,\n    rosenbrock_hessian,\n    LoggingFunction,\n    CountingFunction,\n)\n\nx_min, x_max = -1, 2\ny_min, y_max = 2.25 / 3 * x_min - 0.2, 2.25 / 3 * x_max - 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A formatter to print values on contours\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def super_fmt(value):\n    if value > 1:\n        if np.abs(int(value) - value) < 0.1:\n            out = \"$10^{%.1i}$\" % value\n        else:\n            out = \"$10^{%.1f}$\" % value\n    else:\n        value = np.exp(value - 0.01)\n        if value > 0.1:\n            out = f\"{value:1.1f}\"\n        elif value > 0.01:\n            out = f\"{value:.2f}\"\n        else:\n            out = f\"{value:.2e}\"\n    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A gradient descent algorithm\ndo not use: its a toy, use scipy's optimize.fmin_cg\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x0, f, f_prime, hessian=None, adaptative=False):\n    x_i, y_i = x0\n    all_x_i = []\n    all_y_i = []\n    all_f_i = []\n\n    for i in range(1, 100):\n        all_x_i.append(x_i)\n        all_y_i.append(y_i)\n        all_f_i.append(f([x_i, y_i]))\n        dx_i, dy_i = f_prime(np.asarray([x_i, y_i]))\n        if adaptative:\n            # Compute a step size using a line_search to satisfy the Wolf\n            # conditions\n            step = sp.optimize.line_search(\n                f,\n                f_prime,\n                np.r_[x_i, y_i],\n                -np.r_[dx_i, dy_i],\n                np.r_[dx_i, dy_i],\n                c2=0.05,\n            )\n            step = step[0]\n            if step is None:\n                step = 0\n        else:\n            step = 1\n        x_i += -step * dx_i\n        y_i += -step * dy_i\n        if np.abs(all_f_i[-1]) < 1e-16:\n            break\n    return all_x_i, all_y_i, all_f_i\n\n\ndef gradient_descent_adaptative(x0, f, f_prime, hessian=None):\n    return gradient_descent(x0, f, f_prime, adaptative=True)\n\n\ndef conjugate_gradient(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n\n    sp.optimize.minimize(\n        f, x0, jac=f_prime, method=\"CG\", callback=store, options={\"gtol\": 1e-12}\n    )\n    return all_x_i, all_y_i, all_f_i\n\n\ndef newton_cg(x0, f, f_prime, hessian):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n\n    sp.optimize.minimize(\n        f,\n        x0,\n        method=\"Newton-CG\",\n        jac=f_prime,\n        hess=hessian,\n        callback=store,\n        options={\"xtol\": 1e-12},\n    )\n    return all_x_i, all_y_i, all_f_i\n\n\ndef bfgs(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n\n    sp.optimize.minimize(\n        f, x0, method=\"BFGS\", jac=f_prime, callback=store, options={\"gtol\": 1e-12}\n    )\n    return all_x_i, all_y_i, all_f_i\n\n\ndef powell(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n\n    sp.optimize.minimize(\n        f, x0, method=\"Powell\", callback=store, options={\"ftol\": 1e-12}\n    )\n    return all_x_i, all_y_i, all_f_i\n\n\ndef nelder_mead(x0, f, f_prime, hessian=None):\n    all_x_i = [x0[0]]\n    all_y_i = [x0[1]]\n    all_f_i = [f(x0)]\n\n    def store(X):\n        x, y = X\n        all_x_i.append(x)\n        all_y_i.append(y)\n        all_f_i.append(f(X))\n\n    sp.optimize.minimize(\n        f, x0, method=\"Nelder-Mead\", callback=store, options={\"ftol\": 1e-12}\n    )\n    return all_x_i, all_y_i, all_f_i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run different optimizers on these problems\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "levels = {}\n\nfor index, ((f, f_prime, hessian), optimizer) in enumerate(\n    (\n        (mk_quad(0.7), gradient_descent),\n        (mk_quad(0.7), gradient_descent_adaptative),\n        (mk_quad(0.02), gradient_descent),\n        (mk_quad(0.02), gradient_descent_adaptative),\n        (mk_gauss(0.02), gradient_descent_adaptative),\n        (\n            (rosenbrock, rosenbrock_prime, rosenbrock_hessian),\n            gradient_descent_adaptative,\n        ),\n        (mk_gauss(0.02), conjugate_gradient),\n        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), conjugate_gradient),\n        (mk_quad(0.02), newton_cg),\n        (mk_gauss(0.02), newton_cg),\n        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), newton_cg),\n        (mk_quad(0.02), bfgs),\n        (mk_gauss(0.02), bfgs),\n        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), bfgs),\n        (mk_quad(0.02), powell),\n        (mk_gauss(0.02), powell),\n        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), powell),\n        (mk_gauss(0.02), nelder_mead),\n        ((rosenbrock, rosenbrock_prime, rosenbrock_hessian), nelder_mead),\n    )\n):\n    # Compute a gradient-descent\n    x_i, y_i = 1.6, 1.1\n    counting_f_prime = CountingFunction(f_prime)\n    counting_hessian = CountingFunction(hessian)\n    logging_f = LoggingFunction(f, counter=counting_f_prime.counter)\n    all_x_i, all_y_i, all_f_i = optimizer(\n        np.array([x_i, y_i]), logging_f, counting_f_prime, hessian=counting_hessian\n    )\n\n    # Plot the contour plot\n    if not max(all_y_i) < y_max:\n        x_min *= 1.2\n        x_max *= 1.2\n        y_min *= 1.2\n        y_max *= 1.2\n    x, y = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n    x = x.T\n    y = y.T\n\n    plt.figure(index, figsize=(3, 2.5))\n    plt.clf()\n    plt.axes([0, 0, 1, 1])\n\n    X = np.concatenate((x[np.newaxis, ...], y[np.newaxis, ...]), axis=0)\n    z = np.apply_along_axis(f, 0, X)\n    log_z = np.log(z + 0.01)\n    plt.imshow(\n        log_z,\n        extent=[x_min, x_max, y_min, y_max],\n        cmap=plt.cm.gray_r,\n        origin=\"lower\",\n        vmax=log_z.min() + 1.5 * log_z.ptp(),\n    )\n    contours = plt.contour(\n        log_z,\n        levels=levels.get(f, None),\n        extent=[x_min, x_max, y_min, y_max],\n        cmap=plt.cm.gnuplot,\n        origin=\"lower\",\n    )\n    levels[f] = contours.levels\n    plt.clabel(contours, inline=1, fmt=super_fmt, fontsize=14)\n\n    plt.plot(all_x_i, all_y_i, \"b-\", linewidth=2)\n    plt.plot(all_x_i, all_y_i, \"k+\")\n\n    plt.plot(logging_f.all_x_i, logging_f.all_y_i, \"k.\", markersize=2)\n\n    plt.plot([0], [0], \"rx\", markersize=12)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n    plt.draw()\n\n    plt.figure(index + 100, figsize=(4, 3))\n    plt.clf()\n    plt.semilogy(np.maximum(np.abs(all_f_i), 1e-30), linewidth=2, label=\"# iterations\")\n    plt.ylabel(\"Error on f(x)\")\n    plt.semilogy(\n        logging_f.counts,\n        np.maximum(np.abs(logging_f.all_f_i), 1e-30),\n        linewidth=2,\n        color=\"g\",\n        label=\"# function calls\",\n    )\n    plt.legend(\n        loc=\"upper right\",\n        frameon=True,\n        prop={\"size\": 11},\n        borderaxespad=0,\n        handlelength=1.5,\n        handletextpad=0.5,\n    )\n    plt.tight_layout()\n    plt.draw()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2.7. Mathematical optimization: finding minima of functions &#8212; Scientific Python Lectures</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/nature.css?v=c29d3f28" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="https://unpkg.com/purecss@3.0.0/build/base-min.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../../_static/foldable_toc.css?v=38a22cee" />
    <script src="../../_static/documentation_options.js?v=3d051f0c"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=0729d509"></script>
    <script src="https://code.jquery.com/jquery-3.7.0.min.js"></script>
    <script src="../../_static/scroll_highlight_toc.js?v=cbac5d29"></script>
    <script src="../../_static/foldable_toc.js?v=d65ca516"></script>
    <link rel="icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.7.4.1. Noisy optimization problem" href="auto_examples/plot_noisy.html" />
    <link rel="prev" title="2.6.8.24. Segmentation with spectral clustering" href="../image_processing/auto_examples/plot_spectral_clustering.html" />
 

<script defer data-domain="lectures.scientific-python.org" src="https://views.scientific-python.org/js/script.js"></script>

  </head><body>
    <!-- Use the header to add javascript -->
    

    <script type="text/javascript">
     // Function to toggle tip div
     function toggle_tip_div(obj) {
       $(obj).find("p.summary").remove();

       var content = $(obj).text();
       var html = $(obj).html();

       if ($(obj).hasClass("collapsed")) {
         // Expand
         $(obj).html('<p class="summary"><img src="../../_static/minus.png"></p>' + html);
       } else {
         // Collapse
         if(content.length > 50) {
           content = content.substr(0, 47);
         }
         $(obj).html('<p class="summary"><img src="../../_static/plus.png">' + content + '...</p>' + html);
       }

       $(obj).toggleClass("collapsed");
     }

     $(document).ready(function () {
       $(".tip").each(function() {
         $(this).find("p.admonition-title").remove();
       });
       // Collapse all tips and add plus sign
       $(".tip").each(function() { toggle_tip_div($(this)); });

       $(".tip")
         .click(function(event){
           // Change state of the global button
           $('div.related li.transparent').removeClass('transparent');
           toggle_tip_div($(this));
           if (event.target.tagName.toLowerCase() != "a") {
             return true; //Makes links clickable
           }
         });
     });
    </script>


    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="auto_examples/plot_noisy.html" title="2.7.4.1. Noisy optimization problem"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="../image_processing/auto_examples/plot_spectral_clustering.html" title="2.6.8.24. Segmentation with spectral clustering"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">Scientific Python Lectures</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U"><span class="section-number">2. </span>Advanced topics</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">2.7. </span>Mathematical optimization: finding minima of functions</a></li>
     
    <!-- Insert a tip toggle in the navigation bar -->
        <li class="left">
        <!-- On click, expand all tips -->
        <!--
        <a>
            <img src="../../_static/plus.png"
                 alt="Expand tips" style="padding: 1ex;"/>
            <span class="hiddenlink">Collapse document to compact view</span>
        </a>
        -->
        </li>
    <li class="right edit_on_github"><a href="https://github.com/scipy-lectures/scientific-python-lectures/edit/main/advanced/mathematical_optimization/index.rst">Edit
      <span class="tooltip">
        Improve this page:<br/>Edit it on Github.
      </span>
    </a>
    </li>
    
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="mathematical-optimization-finding-minima-of-functions">
<span id="mathematical-optimization"></span><h1><span class="section-number">2.7. </span>Mathematical optimization: finding minima of functions<a class="headerlink" href="#mathematical-optimization-finding-minima-of-functions" title="Link to this heading">¶</a></h1>
<p><strong>Authors</strong>: <em>Gaël Varoquaux</em></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Mathematical_optimization">Mathematical optimization</a> deals with the
problem of finding numerically minimums (or maximums or zeros) of
a function. In this context, the function is called <em>cost function</em>, or
<em>objective function</em>, or <em>energy</em>.</p>
<p>Here, we are interested in using <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize" title="(in SciPy v1.12.0)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scipy.optimize</span></code></a> for black-box
optimization: we do not rely on the mathematical expression of the
function that we are optimizing. Note that this expression can often be
used for more efficient, non black-box, optimization.</p>
<aside class="topic">
<p class="topic-title">Prerequisites</p>
<ul class="horizontal simple">
<li><p><a class="reference internal" href="../../intro/numpy/index.html#numpy"><span class="std std-ref">NumPy</span></a></p></li>
<li><p><a class="reference internal" href="../../intro/scipy/index.html#scipy"><span class="std std-ref">SciPy</span></a></p></li>
<li><p><a class="reference internal" href="../../intro/matplotlib/index.html#matplotlib"><span class="std std-ref">Matplotlib</span></a></p></li>
</ul>
</aside>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><strong>References</strong></p>
<p>Mathematical optimization is very … mathematical. If you want
performance, it really pays to read the books:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://web.stanford.edu/~boyd/cvxbook/">Convex Optimization</a>
by Boyd and Vandenberghe (pdf available free online).</p></li>
<li><p><a class="reference external" href="https://users.eecs.northwestern.edu/~nocedal/book/num-opt.html">Numerical Optimization</a>,
by Nocedal and Wright. Detailed reference on gradient descent methods.</p></li>
<li><p><a class="reference external" href="https://www.amazon.com/gp/product/0471494631/ref=ox_sc_act_title_1?ie=UTF8&amp;smid=ATVPDKIKX0DER">Practical Methods of Optimization</a> by Fletcher: good at hand-waving explanations.</p></li>
</ul>
</div>
<style type="text/css">
  div.bodywrapper blockquote {
      margin: 0 ;
  }

  div.toctree-wrapper ul {
      margin-top: 0 ;
      margin-bottom: 0 ;
      padding-left: 10px ;
  }

  li.toctree-l1 {
      padding: 0 0 0.5em 0 ;
      list-style-type: none;
      font-size: 150% ;
      font-weight: bold;
      }

  li.toctree-l1 ul {
      padding-left: 40px ;
  }

  li.toctree-l2 {
      font-size: 70% ;
      list-style-type: square;
      font-weight: normal;
      }

  li.toctree-l3 {
      font-size: 85% ;
      list-style-type: circle;
      font-weight: normal;
      }

</style><nav class="contents local" id="chapters-contents">
<p class="topic-title">Chapters contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#knowing-your-problem" id="id10">Knowing your problem</a></p>
<ul>
<li><p><a class="reference internal" href="#convex-versus-non-convex-optimization" id="id11">Convex versus non-convex optimization</a></p></li>
<li><p><a class="reference internal" href="#smooth-and-non-smooth-problems" id="id12">Smooth and non-smooth problems</a></p></li>
<li><p><a class="reference internal" href="#noisy-versus-exact-cost-functions" id="id13">Noisy versus exact cost functions</a></p></li>
<li><p><a class="reference internal" href="#constraints" id="id14">Constraints</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#a-review-of-the-different-optimizers" id="id15">A review of the different optimizers</a></p>
<ul>
<li><p><a class="reference internal" href="#getting-started-1d-optimization" id="id16">Getting started: 1D optimization</a></p></li>
<li><p><a class="reference internal" href="#gradient-based-methods" id="id17">Gradient based methods</a></p></li>
<li><p><a class="reference internal" href="#newton-and-quasi-newton-methods" id="id18">Newton and quasi-newton methods</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#full-code-examples" id="id19">Full code examples</a></p></li>
<li><p><a class="reference internal" href="#examples-for-the-mathematical-optimization-chapter" id="id20">Examples for the mathematical optimization chapter</a></p>
<ul>
<li><p><a class="reference internal" href="#gradient-less-methods" id="id21">Gradient-less methods</a></p></li>
<li><p><a class="reference internal" href="#global-optimizers" id="id22">Global optimizers</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#practical-guide-to-optimization-with-scipy" id="id23">Practical guide to optimization with SciPy</a></p>
<ul>
<li><p><a class="reference internal" href="#choosing-a-method" id="id24">Choosing a method</a></p></li>
<li><p><a class="reference internal" href="#making-your-optimizer-faster" id="id25">Making your optimizer faster</a></p></li>
<li><p><a class="reference internal" href="#computing-gradients" id="id26">Computing gradients</a></p></li>
<li><p><a class="reference internal" href="#synthetic-exercices" id="id27">Synthetic exercices</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#special-case-non-linear-least-squares" id="id28">Special case: non-linear least-squares</a></p>
<ul>
<li><p><a class="reference internal" href="#minimizing-the-norm-of-a-vector-function" id="id29">Minimizing the norm of a vector function</a></p></li>
<li><p><a class="reference internal" href="#curve-fitting" id="id30">Curve fitting</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#optimization-with-constraints" id="id31">Optimization with constraints</a></p>
<ul>
<li><p><a class="reference internal" href="#box-bounds" id="id32">Box bounds</a></p></li>
<li><p><a class="reference internal" href="#general-constraints" id="id33">General constraints</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#id2" id="id34">Full code examples</a></p></li>
<li><p><a class="reference internal" href="#id3" id="id35">Examples for the mathematical optimization chapter</a></p></li>
</ul>
</nav>
<section id="knowing-your-problem">
<h2><a class="toc-backref" href="#id10" role="doc-backlink"><span class="section-number">2.7.1. </span>Knowing your problem</a><a class="headerlink" href="#knowing-your-problem" title="Link to this heading">¶</a></h2>
<p>Not all optimization problems are equal. Knowing your problem enables you
to choose the right tool.</p>
<aside class="topic">
<p class="topic-title"><strong>Dimensionality of the problem</strong></p>
<p>The scale of an optimization problem is pretty much set by the
<em>dimensionality of the problem</em>, i.e. the number of scalar variables
on which the search is performed.</p>
</aside>
<section id="convex-versus-non-convex-optimization">
<h3><a class="toc-backref" href="#id11" role="doc-backlink"><span class="section-number">2.7.1.1. </span>Convex versus non-convex optimization</a><a class="headerlink" href="#convex-versus-non-convex-optimization" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><img alt="convex_1d_1" src="../../_images/sphx_glr_plot_convex_001.png" /></p></td>
<td><p><img alt="convex_1d_2" src="../../_images/sphx_glr_plot_convex_002.png" /></p></td>
</tr>
<tr class="row-even"><td><p><strong>A convex function</strong>:</p>
<ul class="simple">
<li><p><cite>f</cite> is above all its tangents.</p></li>
<li><p>equivalently, for two point A, B, f(C) lies below the segment
[f(A), f(B])], if A &lt; C &lt; B</p></li>
</ul>
</td>
<td><p><strong>A non-convex function</strong></p></td>
</tr>
</tbody>
</table>
<p><strong>Optimizing convex functions is easy. Optimizing non-convex functions can
be very hard.</strong></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It can be proven that for a convex function a local minimum is
also a global minimum. Then, in some sense, the minimum is unique.</p>
</div>
</section>
<section id="smooth-and-non-smooth-problems">
<h3><a class="toc-backref" href="#id12" role="doc-backlink"><span class="section-number">2.7.1.2. </span>Smooth and non-smooth problems</a><a class="headerlink" href="#smooth-and-non-smooth-problems" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><img alt="smooth_1d_1" src="../../_images/sphx_glr_plot_smooth_001.png" /></p></td>
<td><p><img alt="smooth_1d_2" src="../../_images/sphx_glr_plot_smooth_002.png" /></p></td>
</tr>
<tr class="row-even"><td><p><strong>A smooth function</strong>:</p>
<p>The gradient is defined everywhere, and is a continuous function</p>
</td>
<td><p><strong>A non-smooth function</strong></p></td>
</tr>
</tbody>
</table>
<p><strong>Optimizing smooth functions is easier</strong>
(true in the context of <em>black-box</em> optimization, otherwise
<a class="reference external" href="https://en.wikipedia.org/wiki/Linear_programming">Linear Programming</a>
is an example of methods which deal very efficiently with
piece-wise linear functions).</p>
</section>
<section id="noisy-versus-exact-cost-functions">
<h3><a class="toc-backref" href="#id13" role="doc-backlink"><span class="section-number">2.7.1.3. </span>Noisy versus exact cost functions</a><a class="headerlink" href="#noisy-versus-exact-cost-functions" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Noisy (blue) and non-noisy (green) functions</p></td>
<td><p><img alt="noisy" src="../../_images/sphx_glr_plot_noisy_001.png" /></p></td>
</tr>
</tbody>
</table>
<aside class="topic">
<p class="topic-title"><strong>Noisy gradients</strong></p>
<p>Many optimization methods rely on gradients of the objective function.
If the gradient function is not given, they are computed numerically,
which induces errors. In such situation, even if the objective
function is not noisy, a gradient-based optimization may be a noisy
optimization.</p>
</aside>
</section>
<section id="constraints">
<h3><a class="toc-backref" href="#id14" role="doc-backlink"><span class="section-number">2.7.1.4. </span>Constraints</a><a class="headerlink" href="#constraints" title="Link to this heading">¶</a></h3>
<table class="docutils align-default">
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Optimizations under constraints</p>
<p>Here:</p>
<p><img class="math" src="../../_images/math/a4fc69b4f7ef953d0ec943e157b642d26a714ef2.png" alt="-1 &lt; x_1 &lt; 1"/></p>
<p><img class="math" src="../../_images/math/4e881611ce180e2b4c74ff985ec1679851ccead0.png" alt="-1 &lt; x_2 &lt; 1"/></p>
</td>
<td><p><a class="reference external" href="auto_examples/plot_constraints.html"><img alt="constraints" src="../../_images/sphx_glr_plot_constraints_001.png" /></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="a-review-of-the-different-optimizers">
<h2><a class="toc-backref" href="#id15" role="doc-backlink"><span class="section-number">2.7.2. </span>A review of the different optimizers</a><a class="headerlink" href="#a-review-of-the-different-optimizers" title="Link to this heading">¶</a></h2>
<section id="getting-started-1d-optimization">
<h3><a class="toc-backref" href="#id16" role="doc-backlink"><span class="section-number">2.7.2.1. </span>Getting started: 1D optimization</a><a class="headerlink" href="#getting-started-1d-optimization" title="Link to this heading">¶</a></h3>
<p>Let’s get started by finding the minimum of the scalar function
<img class="math" src="../../_images/math/824f83337f9ce5be12d611208fb84c8540fc425f.png" alt="f(x)=\exp[(x-0.7)^2]"/>. <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize_scalar()</span></code></a> uses
Brent’s method to find the minimum of a function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize_scalar</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="o">.</span><span class="n">success</span> <span class="c1"># check if solver was successful</span>
<div class="newline"></div><span class="go">True</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x_min</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x_min</span>
<div class="newline"></div><span class="go">0.50...</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x_min</span> <span class="o">-</span> <span class="mf">0.5</span>
<div class="newline"></div><span class="go">5.8...e-09</span>
<div class="newline"></div></pre></div>
</div>
<table class="docutils align-default" id="id5">
<caption><span class="caption-text"><strong>Brent’s method on a quadratic function</strong>: it
             converges in 3 iterations, as the quadratic
             approximation is then exact.</span><a class="headerlink" href="#id5" title="Link to this table">¶</a></caption>
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="../../_images/sphx_glr_plot_1d_optim_001.png"><img alt="1d_optim_1" src="../../_images/sphx_glr_plot_1d_optim_001.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_1d_optim_002.png"><img alt="1d_optim_2" src="../../_images/sphx_glr_plot_1d_optim_002.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id6">
<caption><span class="caption-text"><strong>Brent’s method on a non-convex function</strong>: note that
             the fact that the optimizer avoided the local minimum
             is a matter of luck.</span><a class="headerlink" href="#id6" title="Link to this table">¶</a></caption>
<colgroup>
<col style="width: 50.0%" />
<col style="width: 50.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="../../_images/sphx_glr_plot_1d_optim_003.png"><img alt="1d_optim_3" src="../../_images/sphx_glr_plot_1d_optim_003.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_1d_optim_004.png"><img alt="1d_optim_4" src="../../_images/sphx_glr_plot_1d_optim_004.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can use different solvers using the parameter <code class="docutils literal notranslate"><span class="pre">method</span></code>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize_scalar()</span></code></a> can also be used for optimization
constrained to an interval using the parameter <code class="docutils literal notranslate"><span class="pre">bounds</span></code>.</p>
</div>
</section>
<section id="gradient-based-methods">
<h3><a class="toc-backref" href="#id17" role="doc-backlink"><span class="section-number">2.7.2.2. </span>Gradient based methods</a><a class="headerlink" href="#gradient-based-methods" title="Link to this heading">¶</a></h3>
<section id="some-intuitions-about-gradient-descent">
<h4>Some intuitions about gradient descent<a class="headerlink" href="#some-intuitions-about-gradient-descent" title="Link to this heading">¶</a></h4>
<p>Here we focus on <strong>intuitions</strong>, not code. Code will follow.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient descent</a>
basically consists in taking small steps in the direction of the
gradient, that is the direction of the <em>steepest descent</em>.</p>
<table class="docutils align-default" id="id7">
<caption><span class="caption-text"><strong>Fixed step gradient descent</strong></span><a class="headerlink" href="#id7" title="Link to this table">¶</a></caption>
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>A well-conditioned quadratic function.</strong></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_001.png"><img alt="gradient_quad_cond" src="../../_images/sphx_glr_plot_gradient_descent_001.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_020.png"><img alt="gradient_quad_cond_conv" src="../../_images/sphx_glr_plot_gradient_descent_020.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p><strong>An ill-conditioned quadratic function.</strong></p>
<p>The core problem of gradient-methods on ill-conditioned problems is
that the gradient tends not to point in the direction of the
minimum.</p>
</td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_003.png"><img alt="gradient_quad_icond" src="../../_images/sphx_glr_plot_gradient_descent_003.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_022.png"><img alt="gradient_quad_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_022.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<p>We can see that very anisotropic (<a class="reference external" href="https://en.wikipedia.org/wiki/Condition_number">ill-conditioned</a>) functions are harder
to optimize.</p>
<aside class="topic">
<p class="topic-title"><strong>Take home message: conditioning number and preconditioning</strong></p>
<p>If you know natural scaling for your variables, prescale them so that
they behave similarly. This is related to <a class="reference external" href="https://en.wikipedia.org/wiki/Preconditioner">preconditioning</a>.</p>
</aside>
<p>Also, it clearly can be advantageous to take bigger steps. This
is done in gradient descent code using a
<a class="reference external" href="https://en.wikipedia.org/wiki/Line_search">line search</a>.</p>
<table class="docutils align-default" id="id8">
<caption><span class="caption-text"><strong>Adaptive step gradient descent</strong></span><a class="headerlink" href="#id8" title="Link to this table">¶</a></caption>
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>A well-conditioned quadratic function.</p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_002.png"><img alt="agradient_quad_cond" src="../../_images/sphx_glr_plot_gradient_descent_002.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_021.png"><img alt="agradient_quad_cond_conv" src="../../_images/sphx_glr_plot_gradient_descent_021.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>An ill-conditioned quadratic function.</p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_004.png"><img alt="agradient_quad_icond" src="../../_images/sphx_glr_plot_gradient_descent_004.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_023.png"><img alt="agradient_quad_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_023.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p>An ill-conditioned non-quadratic function.</p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_005.png"><img alt="agradient_gauss_icond" src="../../_images/sphx_glr_plot_gradient_descent_005.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_024.png"><img alt="agradient_gauss_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_024.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>An ill-conditioned very non-quadratic function.</p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_006.png"><img alt="agradient_rosen_icond" src="../../_images/sphx_glr_plot_gradient_descent_006.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_025.png"><img alt="agradient_rosen_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_025.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<p>The more a function looks like a quadratic function (elliptic
iso-curves), the easier it is to optimize.</p>
</section>
<section id="conjugate-gradient-descent">
<h4>Conjugate gradient descent<a class="headerlink" href="#conjugate-gradient-descent" title="Link to this heading">¶</a></h4>
<p>The gradient descent algorithms above are toys not to be used on real
problems.</p>
<p>As can be seen from the above experiments, one of the problems of the
simple gradient descent algorithms, is that it tends to oscillate across
a valley, each time following the direction of the gradient, that makes
it cross the valley. The conjugate gradient solves this problem by adding
a <em>friction</em> term: each step depends on the two last values of the
gradient and sharp turns are reduced.</p>
<table class="docutils align-default" id="id9">
<caption><span class="caption-text"><strong>Conjugate gradient descent</strong></span><a class="headerlink" href="#id9" title="Link to this table">¶</a></caption>
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>An ill-conditioned non-quadratic function.</p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_007.png"><img alt="cg_gauss_icond" src="../../_images/sphx_glr_plot_gradient_descent_007.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_026.png"><img alt="cg_gauss_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_026.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p>An ill-conditioned very non-quadratic function.</p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_008.png"><img alt="cg_rosen_icond" src="../../_images/sphx_glr_plot_gradient_descent_008.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_027.png"><img alt="cg_rosen_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_027.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<p>SciPy provides <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize()</span></code></a> to find the minimum of scalar
functions of one or more variables. The simple conjugate gradient method can
be used by setting the parameter <code class="docutils literal notranslate"><span class="pre">method</span></code> to CG</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>   <span class="c1"># The rosenbrock function</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;CG&quot;</span><span class="p">)</span>
<div class="newline"></div><span class="go"> message: Optimization terminated successfully.</span>
<div class="newline"></div><span class="go"> success: True</span>
<div class="newline"></div><span class="go">  status: 0</span>
<div class="newline"></div><span class="go">     fun: 1.650...e-11</span>
<div class="newline"></div><span class="go">       x: [ 1.000e+00  1.000e+00]</span>
<div class="newline"></div><span class="go">     nit: 13</span>
<div class="newline"></div><span class="go">     jac: [-6.15...e-06  2.53...e-07]</span>
<div class="newline"></div><span class="go">    nfev: 81</span>
<div class="newline"></div><span class="go">    njev: 27</span>
<div class="newline"></div></pre></div>
</div>
<p>Gradient methods need the Jacobian (gradient) of the function. They can compute it
numerically, but will perform better if you can pass them the gradient:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;CG&quot;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jacobian</span><span class="p">)</span>
<div class="newline"></div><span class="go"> message: Optimization terminated successfully.</span>
<div class="newline"></div><span class="go"> success: True</span>
<div class="newline"></div><span class="go">  status: 0</span>
<div class="newline"></div><span class="go">     fun: 2.95786...e-14</span>
<div class="newline"></div><span class="go">       x: [ 1.000e+00  1.000e+00]</span>
<div class="newline"></div><span class="go">     nit: 8</span>
<div class="newline"></div><span class="go">     jac: [ 7.183e-07 -2.990e-07]</span>
<div class="newline"></div><span class="go">    nfev: 16</span>
<div class="newline"></div><span class="go">    njev: 16</span>
<div class="newline"></div></pre></div>
</div>
<p>Note that the function has only been evaluated 27 times, compared to 108
without the gradient.</p>
</section>
</section>
<section id="newton-and-quasi-newton-methods">
<h3><a class="toc-backref" href="#id18" role="doc-backlink"><span class="section-number">2.7.2.3. </span>Newton and quasi-newton methods</a><a class="headerlink" href="#newton-and-quasi-newton-methods" title="Link to this heading">¶</a></h3>
<section id="newton-methods-using-the-hessian-2nd-differential">
<h4>Newton methods: using the Hessian (2nd differential)<a class="headerlink" href="#newton-methods-using-the-hessian-2nd-differential" title="Link to this heading">¶</a></h4>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">Newton methods</a> use a
local quadratic approximation to compute the jump direction. For this
purpose, they rely on the 2 first derivative of the function: the
<em>gradient</em> and the <a class="reference external" href="https://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>An ill-conditioned quadratic function:</strong></p>
<p>Note that, as the quadratic approximation is exact, the Newton
method is blazing fast</p>
</td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_009.png"><img alt="ncg_quad_icond" src="../../_images/sphx_glr_plot_gradient_descent_009.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_028.png"><img alt="ncg_quad_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_028.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p><strong>An ill-conditioned non-quadratic function:</strong></p>
<p>Here we are optimizing a Gaussian, which is always below its
quadratic approximation. As a result, the Newton method overshoots
and leads to oscillations.</p>
</td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_010.png"><img alt="ncg_gauss_icond" src="../../_images/sphx_glr_plot_gradient_descent_010.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_029.png"><img alt="ncg_gauss_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_029.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p><strong>An ill-conditioned very non-quadratic function:</strong></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_011.png"><img alt="ncg_rosen_icond" src="../../_images/sphx_glr_plot_gradient_descent_011.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_030.png"><img alt="ncg_rosen_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_030.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<p>In SciPy, you can use the Newton method by setting <code class="docutils literal notranslate"><span class="pre">method</span></code> to Newton-CG in
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize()</span></code></a>. Here, CG refers to the fact that an internal
inversion of the Hessian is performed by conjugate gradient</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>   <span class="c1"># The rosenbrock function</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;Newton-CG&quot;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jacobian</span><span class="p">)</span>
<div class="newline"></div><span class="go"> message: Optimization terminated successfully.</span>
<div class="newline"></div><span class="go"> success: True</span>
<div class="newline"></div><span class="go">  status: 0</span>
<div class="newline"></div><span class="go">     fun: 1.5601357400786612e-15</span>
<div class="newline"></div><span class="go">       x: [ 1.000e+00  1.000e+00]</span>
<div class="newline"></div><span class="go">     nit: 10</span>
<div class="newline"></div><span class="go">     jac: [ 1.058e-07 -7.483e-08]</span>
<div class="newline"></div><span class="go">    nfev: 11</span>
<div class="newline"></div><span class="go">    njev: 33</span>
<div class="newline"></div><span class="go">    nhev: 0</span>
<div class="newline"></div></pre></div>
</div>
<p>Note that compared to a conjugate gradient (above), Newton’s method has
required less function evaluations, but more gradient evaluations, as it
uses it to approximate the Hessian. Let’s compute the Hessian and pass it
to the algorithm:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="c1"># Computed with sympy</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(((</span><span class="mi">1</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">12</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">)))</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;Newton-CG&quot;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jacobian</span><span class="p">,</span> <span class="n">hess</span><span class="o">=</span><span class="n">hessian</span><span class="p">)</span>
<div class="newline"></div><span class="go"> message: Optimization terminated successfully.</span>
<div class="newline"></div><span class="go"> success: True</span>
<div class="newline"></div><span class="go">  status: 0</span>
<div class="newline"></div><span class="go">     fun: 1.6277298383706738e-15</span>
<div class="newline"></div><span class="go">       x: [ 1.000e+00  1.000e+00]</span>
<div class="newline"></div><span class="go">     nit: 10</span>
<div class="newline"></div><span class="go">     jac: [ 1.110e-07 -7.781e-08]</span>
<div class="newline"></div><span class="go">    nfev: 11</span>
<div class="newline"></div><span class="go">    njev: 11</span>
<div class="newline"></div><span class="go">    nhev: 10</span>
<div class="newline"></div></pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>At very high-dimension, the inversion of the Hessian can be costly
and unstable (large scale &gt; 250).</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Newton optimizers should not to be confused with Newton’s root finding
method, based on the same principles, <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.newton.html#scipy.optimize.newton" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.newton()</span></code></a>.</p>
</div>
</section>
<section id="quasi-newton-methods-approximating-the-hessian-on-the-fly">
<span id="quasi-newton"></span><h4>Quasi-Newton methods: approximating the Hessian on the fly<a class="headerlink" href="#quasi-newton-methods-approximating-the-hessian-on-the-fly" title="Link to this heading">¶</a></h4>
<p><strong>BFGS</strong>: BFGS (Broyden-Fletcher-Goldfarb-Shanno algorithm) refines at
each step an approximation of the Hessian.</p>
</section>
</section>
</section>
<section id="full-code-examples">
<h2><a class="toc-backref" href="#id19" role="doc-backlink"><span class="section-number">2.7.3. </span>Full code examples</a><a class="headerlink" href="#full-code-examples" title="Link to this heading">¶</a></h2>
</section>
<section id="examples-for-the-mathematical-optimization-chapter">
<h2><a class="toc-backref" href="#id20" role="doc-backlink"><span class="section-number">2.7.4. </span>Examples for the mathematical optimization chapter</a><a class="headerlink" href="#examples-for-the-mathematical-optimization-chapter" title="Link to this heading">¶</a></h2>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Draws a figure explaining noisy vs non-noisy optimization"><img alt="" src="../../_images/sphx_glr_plot_noisy_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_noisy.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-noisy-py"><span class="std std-ref">Noisy optimization problem</span></a></p>
  <div class="sphx-glr-thumbnail-title">Noisy optimization problem</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Draws a figure to explain smooth versus non smooth optimization."><img alt="" src="../../_images/sphx_glr_plot_smooth_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_smooth.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-smooth-py"><span class="std std-ref">Smooth vs non-smooth</span></a></p>
  <div class="sphx-glr-thumbnail-title">Smooth vs non-smooth</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A curve fitting example"><img alt="" src="../../_images/sphx_glr_plot_curve_fitting_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_curve_fitting.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-curve-fitting-py"><span class="std std-ref">Curve fitting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Curve fitting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A figure showing the definition of a convex function"><img alt="" src="../../_images/sphx_glr_plot_convex_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_convex.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-convex-py"><span class="std std-ref">Convex function</span></a></p>
  <div class="sphx-glr-thumbnail-title">Convex function</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An exercise of finding minimum. This exercise is hard because the function is very flat around ..."><img alt="" src="../../_images/sphx_glr_plot_exercise_flat_minimum_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_exercise_flat_minimum.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-exercise-flat-minimum-py"><span class="std std-ref">Finding a minimum in a flat neighborhood</span></a></p>
  <div class="sphx-glr-thumbnail-title">Finding a minimum in a flat neighborhood</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example showing how to do optimization with general constraints using SLSQP and cobyla."><img alt="" src="../../_images/sphx_glr_plot_non_bounds_constraints_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_non_bounds_constraints.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-non-bounds-constraints-py"><span class="std std-ref">Optimization with constraints</span></a></p>
  <div class="sphx-glr-thumbnail-title">Optimization with constraints</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Illustration of 1D optimization: Brent&#x27;s method"><img alt="" src="../../_images/sphx_glr_plot_1d_optim_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_1d_optim.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-1d-optim-py"><span class="std std-ref">Brent’s method</span></a></p>
  <div class="sphx-glr-thumbnail-title">Brent's method</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A small figure explaining optimization with constraints"><img alt="" src="../../_images/sphx_glr_plot_constraints_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_constraints.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-constraints-py"><span class="std std-ref">Constraint optimization: visualizing the geometry</span></a></p>
  <div class="sphx-glr-thumbnail-title">Constraint optimization: visualizing the geometry</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plots the results from the comparison of optimizers."><img alt="" src="../../_images/sphx_glr_plot_compare_optimizers_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_compare_optimizers.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-compare-optimizers-py"><span class="std std-ref">Plotting the comparison of optimizers</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plotting the comparison of optimizers</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The challenge here is that Hessian of the problem is a very ill-conditioned matrix. This can ea..."><img alt="" src="../../_images/sphx_glr_plot_exercise_ill_conditioned_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_exercise_ill_conditioned.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-exercise-ill-conditioned-py"><span class="std std-ref">Alternating optimization</span></a></p>
  <div class="sphx-glr-thumbnail-title">Alternating optimization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example demoing gradient descent by creating figures that trace the evolution of the optimiz..."><img alt="" src="../../_images/sphx_glr_plot_gradient_descent_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_gradient_descent.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-gradient-descent-py"><span class="std std-ref">Gradient descent</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gradient descent</div>
</div></div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-footer sphx-glr-footer-gallery docutils container">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/2250da8939d81b5cc457b85593005b83/auto_examples_python.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">auto_examples_python.zip</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/b4f7dfe5dea520e69bca7a7b06d7ba42/auto_examples_jupyter.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Jupyter</span> <span class="pre">notebooks:</span> <span class="pre">auto_examples_jupyter.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>An ill-conditioned quadratic function:</strong></p>
<p>On a exactly quadratic function, BFGS is not as fast as Newton’s
method, but still very fast.</p>
</td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_012.png"><img alt="bfgs_quad_icond" src="../../_images/sphx_glr_plot_gradient_descent_012.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_031.png"><img alt="bfgs_quad_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_031.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p><strong>An ill-conditioned non-quadratic function:</strong></p>
<p>Here BFGS does better than Newton, as its empirical estimate of the
curvature is better than that given by the Hessian.</p>
</td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_013.png"><img alt="bfgs_gauss_icond" src="../../_images/sphx_glr_plot_gradient_descent_013.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_032.png"><img alt="bfgs_gauss_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_032.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-odd"><td><p><strong>An ill-conditioned very non-quadratic function:</strong></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_014.png"><img alt="bfgs_rosen_icond" src="../../_images/sphx_glr_plot_gradient_descent_014.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_033.png"><img alt="bfgs_rosen_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_033.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>   <span class="c1"># The rosenbrock function</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;BFGS&quot;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jacobian</span><span class="p">)</span>
<div class="newline"></div><span class="go"> message: Optimization terminated successfully.</span>
<div class="newline"></div><span class="go"> success: True</span>
<div class="newline"></div><span class="go">  status: 0</span>
<div class="newline"></div><span class="go">     fun: 2.630637192365927e-16</span>
<div class="newline"></div><span class="go">       x: [ 1.000e+00  1.000e+00]</span>
<div class="newline"></div><span class="go">     nit: 8</span>
<div class="newline"></div><span class="go">     jac: [ 6.709e-08 -3.222e-08]</span>
<div class="newline"></div><span class="go">hess_inv: [[ 9.999e-01  2.000e+00]</span>
<div class="newline"></div><span class="go">           [ 2.000e+00  4.499e+00]]</span>
<div class="newline"></div><span class="go">    nfev: 10</span>
<div class="newline"></div><span class="go">    njev: 10</span>
<div class="newline"></div></pre></div>
</div>
<p><strong>L-BFGS:</strong> Limited-memory BFGS Sits between BFGS and conjugate gradient:
in very high dimensions (&gt; 250) the Hessian matrix is too costly to
compute and invert. L-BFGS keeps a low-rank version. In addition, box bounds
are also supported by L-BFGS-B:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>   <span class="c1"># The rosenbrock function</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">jacobian</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;L-BFGS-B&quot;</span><span class="p">,</span> <span class="n">jac</span><span class="o">=</span><span class="n">jacobian</span><span class="p">)</span>
<div class="newline"></div><span class="go"> message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL</span>
<div class="newline"></div><span class="go"> success: True</span>
<div class="newline"></div><span class="go">  status: 0</span>
<div class="newline"></div><span class="go">     fun: 1.4417677473...e-15</span>
<div class="newline"></div><span class="go">       x: [ 1.000e+00  1.000e+00]</span>
<div class="newline"></div><span class="go">     nit: 16</span>
<div class="newline"></div><span class="go">     jac: [ 1.023e-07 -2.593e-08]</span>
<div class="newline"></div><span class="go">    nfev: 17</span>
<div class="newline"></div><span class="go">    njev: 17</span>
<div class="newline"></div><span class="go">hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;</span>
<div class="newline"></div></pre></div>
</div>
<section id="gradient-less-methods">
<h3><a class="toc-backref" href="#id21" role="doc-backlink"><span class="section-number">2.7.4.12. </span>Gradient-less methods</a><a class="headerlink" href="#gradient-less-methods" title="Link to this heading">¶</a></h3>
<section id="a-shooting-method-the-powell-algorithm">
<h4>A shooting method: the Powell algorithm<a class="headerlink" href="#a-shooting-method-the-powell-algorithm" title="Link to this heading">¶</a></h4>
<p>Almost a gradient approach</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>An ill-conditioned quadratic function:</strong></p>
<p>Powell’s method isn’t too sensitive to local ill-conditionning in
low dimensions</p>
</td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_015.png"><img alt="powell_quad_icond" src="../../_images/sphx_glr_plot_gradient_descent_015.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_034.png"><img alt="powell_quad_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_034.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p><strong>An ill-conditioned very non-quadratic function:</strong></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_017.png"><img alt="powell_rosen_icond" src="../../_images/sphx_glr_plot_gradient_descent_017.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_036.png"><img alt="powell_rosen_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_036.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="simplex-method-the-nelder-mead">
<h4>Simplex method: the Nelder-Mead<a class="headerlink" href="#simplex-method-the-nelder-mead" title="Link to this heading">¶</a></h4>
<p>The Nelder-Mead algorithms is a generalization of dichotomy approaches to
high-dimensional spaces. The algorithm works by refining a <a class="reference external" href="https://en.wikipedia.org/wiki/Simplex">simplex</a>, the generalization of intervals
and triangles to high-dimensional spaces, to bracket the minimum.</p>
<p><strong>Strong points</strong>: it is robust to noise, as it does not rely on
computing gradients. Thus it can work on functions that are not locally
smooth such as experimental data points, as long as they display a
large-scale bell-shape behavior. However it is slower than gradient-based
methods on smooth, non-noisy functions.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33.3%" />
<col style="width: 33.3%" />
<col style="width: 33.3%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>An ill-conditioned non-quadratic function:</strong></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_018.png"><img alt="nm_gauss_icond" src="../../_images/sphx_glr_plot_gradient_descent_018.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_037.png"><img alt="nm_gauss_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_037.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
<tr class="row-even"><td><p><strong>An ill-conditioned very non-quadratic function:</strong></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_019.png"><img alt="nm_rosen_icond" src="../../_images/sphx_glr_plot_gradient_descent_019.png" style="width: 270.0px; height: 225.0px;" /></a></p></td>
<td><p><a class="reference internal" href="../../_images/sphx_glr_plot_gradient_descent_038.png"><img alt="nm_rosen_icond_conv" src="../../_images/sphx_glr_plot_gradient_descent_038.png" style="width: 300.0px; height: 225.0px;" /></a></p></td>
</tr>
</tbody>
</table>
<p>Using the Nelder-Mead solver in <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>   <span class="c1"># The rosenbrock function</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;Nelder-Mead&quot;</span><span class="p">)</span>
<div class="newline"></div><span class="go">       message: Optimization terminated successfully.</span>
<div class="newline"></div><span class="go">       success: True</span>
<div class="newline"></div><span class="go">        status: 0</span>
<div class="newline"></div><span class="go">           fun: 1.11527915993744e-10</span>
<div class="newline"></div><span class="go">             x: [ 1.000e+00  1.000e+00]</span>
<div class="newline"></div><span class="go">           nit: 58</span>
<div class="newline"></div><span class="go">          nfev: 111</span>
<div class="newline"></div><span class="go"> final_simplex: (array([[ 1.000e+00,  1.000e+00],</span>
<div class="newline"></div><span class="go">                       [ 1.000e+00,  1.000e+00],</span>
<div class="newline"></div><span class="go">                       [ 1.000e+00,  1.000e+00]]), array([ 1.115e-10,  1.537e-10,  4.988e-10]))</span>
<div class="newline"></div></pre></div>
</div>
</section>
</section>
<section id="global-optimizers">
<h3><a class="toc-backref" href="#id22" role="doc-backlink"><span class="section-number">2.7.4.13. </span>Global optimizers</a><a class="headerlink" href="#global-optimizers" title="Link to this heading">¶</a></h3>
<p>If your problem does not admit a unique local minimum (which can be hard
to test unless the function is convex), and you do not have prior
information to initialize the optimization close to the solution, you
may need a global optimizer.</p>
<section id="brute-force-a-grid-search">
<h4>Brute force: a grid search<a class="headerlink" href="#brute-force-a-grid-search" title="Link to this heading">¶</a></h4>
<p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.brute.html#scipy.optimize.brute" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.brute()</span></code></a> evaluates the function on a given grid of
parameters and returns the parameters corresponding to the minimum
value. The parameters are specified with ranges given to
<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.mgrid.html#numpy.mgrid" title="(in NumPy v1.26)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">numpy.mgrid</span></code></a>. By default, 20 steps are taken in each direction:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>   <span class="c1"># The rosenbrock function</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="mf">.5</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">brute</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span> 
<div class="newline"></div><span class="go">array([1.0000...,  1.0000...])</span>
<div class="newline"></div></pre></div>
</div>
</section>
</section>
</section>
<section id="practical-guide-to-optimization-with-scipy">
<h2><a class="toc-backref" href="#id23" role="doc-backlink"><span class="section-number">2.7.5. </span>Practical guide to optimization with SciPy</a><a class="headerlink" href="#practical-guide-to-optimization-with-scipy" title="Link to this heading">¶</a></h2>
<section id="choosing-a-method">
<h3><a class="toc-backref" href="#id24" role="doc-backlink"><span class="section-number">2.7.5.1. </span>Choosing a method</a><a class="headerlink" href="#choosing-a-method" title="Link to this heading">¶</a></h3>
<p>All methods are exposed as the <code class="docutils literal notranslate"><span class="pre">method</span></code> argument of
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize()</span></code></a>.</p>
<a class="reference internal image-reference" href="../../_images/sphx_glr_plot_compare_optimizers_001.png"><img alt="../../_images/sphx_glr_plot_compare_optimizers_001.png" class="align-center" src="../../_images/sphx_glr_plot_compare_optimizers_001.png" style="width: 95%;" /></a>
<dl class="field-list simple">
<dt class="field-odd">Without knowledge of the gradient<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>In general, prefer <strong>BFGS</strong> or <strong>L-BFGS</strong>, even if you have to approximate
numerically gradients. These are also the default if you omit the parameter
<code class="docutils literal notranslate"><span class="pre">method</span></code> - depending if the problem has constraints or bounds</p></li>
<li><p>On well-conditioned problems, <strong>Powell</strong>
and <strong>Nelder-Mead</strong>, both gradient-free methods, work well in
high dimension, but they collapse for ill-conditioned problems.</p></li>
</ul>
</dd>
<dt class="field-even">With knowledge of the gradient<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>BFGS</strong> or <strong>L-BFGS</strong>.</p></li>
<li><p>Computational overhead of BFGS is larger than that L-BFGS, itself
larger than that of conjugate gradient. On the other side, BFGS usually
needs less function evaluations than CG. Thus conjugate gradient method
is better than BFGS at optimizing computationally cheap functions.</p></li>
</ul>
</dd>
<dt class="field-odd">With the Hessian<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p>If you can compute the Hessian, prefer the Newton method
(<strong>Newton-CG</strong> or <strong>TCG</strong>).</p></li>
</ul>
</dd>
<dt class="field-even">If you have noisy measurements<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p>Use <strong>Nelder-Mead</strong> or <strong>Powell</strong>.</p></li>
</ul>
</dd>
</dl>
</section>
<section id="making-your-optimizer-faster">
<h3><a class="toc-backref" href="#id25" role="doc-backlink"><span class="section-number">2.7.5.2. </span>Making your optimizer faster</a><a class="headerlink" href="#making-your-optimizer-faster" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Choose the right method (see above), do compute analytically the
gradient and Hessian, if you can.</p></li>
<li><p>Use <a class="reference external" href="https://en.wikipedia.org/wiki/Preconditioner">preconditionning</a>
when possible.</p></li>
<li><p>Choose your initialization points wisely. For instance, if you are
running many similar optimizations, warm-restart one with the results of
another.</p></li>
<li><p>Relax the tolerance if you don’t need precision using the parameter <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
</ul>
</section>
<section id="computing-gradients">
<h3><a class="toc-backref" href="#id26" role="doc-backlink"><span class="section-number">2.7.5.3. </span>Computing gradients</a><a class="headerlink" href="#computing-gradients" title="Link to this heading">¶</a></h3>
<p>Computing gradients, and even more Hessians, is very tedious but worth
the effort. Symbolic computation with <a class="reference internal" href="../../packages/sympy.html#sympy"><span class="std std-ref">Sympy</span></a> may come in
handy.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>A <em>very</em> common source of optimization not converging well is human
error in the computation of the gradient. You can use
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.check_grad.html#scipy.optimize.check_grad" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.check_grad()</span></code></a> to check that your gradient is
correct. It returns the norm of the different between the gradient
given, and a gradient computed numerically:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">check_grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">jacobian</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<div class="newline"></div><span class="go">2.384185791015625e-07</span>
<div class="newline"></div></pre></div>
</div>
<p>See also <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.approx_fprime.html#scipy.optimize.approx_fprime" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.approx_fprime()</span></code></a> to find your errors.</p>
</div>
</section>
<section id="synthetic-exercices">
<h3><a class="toc-backref" href="#id27" role="doc-backlink"><span class="section-number">2.7.5.4. </span>Synthetic exercices</a><a class="headerlink" href="#synthetic-exercices" title="Link to this heading">¶</a></h3>
<a class="reference external image-reference" href="auto_examples/plot_exercise_ill_conditioned.html"><img alt="../../_images/sphx_glr_plot_exercise_ill_conditioned_001.png" class="align-right" src="../../_images/sphx_glr_plot_exercise_ill_conditioned_001.png" style="width: 224.0px; height: 168.0px;" /></a>
<aside class="topic green">
<p class="topic-title"><strong>Exercice: A simple (?) quadratic function</strong></p>
<p>Optimize the following function, using K[0] as a starting point:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">27446968</span><span class="p">)</span>
<div class="newline"></div><span class="n">K</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<div class="newline"></div>
<div class="newline"></div><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">K</span> <span class="o">@</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<div class="newline"></div></pre></div>
</div>
<p>Time your approach. Find the fastest approach. Why is BFGS not
working well?</p>
</aside>
<aside class="topic green">
<p class="topic-title"><strong>Exercice: A locally flat minimum</strong></p>
<p>Consider the function <cite>exp(-1/(.1*x**2 + y**2)</cite>. This function admits
a minimum in (0, 0). Starting from an initialization at (1, 1), try
to get within 1e-8 of this minimum point.</p>
<p class="centered">
<strong><a class="reference external" href="auto_examples/plot_exercise_flat_minimum.html"><img alt="flat_min_0" src="../../_images/sphx_glr_plot_exercise_flat_minimum_001.png" style="width: 307.2px; height: 230.39999999999998px;" /></a> <a class="reference external" href="auto_examples/plot_exercise_flat_minimum.html"><img alt="flat_min_1" src="../../_images/sphx_glr_plot_exercise_flat_minimum_002.png" style="width: 307.2px; height: 230.39999999999998px;" /></a></strong></p></aside>
</section>
</section>
<section id="special-case-non-linear-least-squares">
<h2><a class="toc-backref" href="#id28" role="doc-backlink"><span class="section-number">2.7.6. </span>Special case: non-linear least-squares</a><a class="headerlink" href="#special-case-non-linear-least-squares" title="Link to this heading">¶</a></h2>
<section id="minimizing-the-norm-of-a-vector-function">
<h3><a class="toc-backref" href="#id29" role="doc-backlink"><span class="section-number">2.7.6.1. </span>Minimizing the norm of a vector function</a><a class="headerlink" href="#minimizing-the-norm-of-a-vector-function" title="Link to this heading">¶</a></h3>
<p>Least square problems, minimizing the norm of a vector function, have a
specific structure that can be used in the <a class="reference external" href="https://en.wikipedia.org/wiki/Levenberg-Marquardt_algorithm">Levenberg–Marquardt algorithm</a>
implemented in <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html#scipy.optimize.leastsq" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.leastsq()</span></code></a>.</p>
<p>Lets try to minimize the norm of the following vectorial function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">leastsq</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>
<div class="newline"></div><span class="go">(array([0.        ,  0.11111111,  0.22222222,  0.33333333,  0.44444444,</span>
<div class="newline"></div><span class="go">        0.55555556,  0.66666667,  0.77777778,  0.88888889,  1.        ]), 2)</span>
<div class="newline"></div></pre></div>
</div>
<p>This took 67 function evaluations (check it with ‘full_output=1’). What
if we compute the norm ourselves and use a good generic optimizer
(BFGS):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;BFGS&quot;</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">result</span><span class="o">.</span><span class="n">fun</span>
<div class="newline"></div><span class="go">2.6940...e-11</span>
<div class="newline"></div></pre></div>
</div>
<p>BFGS needs more function calls, and gives a less precise result.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><cite>leastsq</cite> is interesting compared to BFGS only if the
dimensionality of the output vector is large, and larger than the number
of parameters to optimize.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If the function is linear, this is a linear-algebra problem, and
should be solved with <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html#scipy.linalg.lstsq" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.linalg.lstsq()</span></code></a>.</p>
</div>
</section>
<section id="curve-fitting">
<h3><a class="toc-backref" href="#id30" role="doc-backlink"><span class="section-number">2.7.6.2. </span>Curve fitting</a><a class="headerlink" href="#curve-fitting" title="Link to this heading">¶</a></h3>
<a class="reference external image-reference" href="auto_examples/plot_curve_fitting.html"><img alt="../../_images/sphx_glr_plot_curve_fitting_001.png" class="align-right" src="../../_images/sphx_glr_plot_curve_fitting_001.png" style="width: 307.2px; height: 230.39999999999998px;" /></a>
<p>Least square problems occur often when fitting a non-linear to data.
While it is possible to construct our optimization problem ourselves,
SciPy provides a helper function for this purpose:
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html#scipy.optimize.curve_fit" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">omega</span><span class="p">,</span> <span class="n">phi</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">omega</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="n">phi</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">27446968</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">.1</span><span class="o">*</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<div class="newline"></div><span class="go">(array([1.4812..., 0.9999...]), array([[ 0.0003..., -0.0004...],</span>
<div class="newline"></div><span class="go">       [-0.0004...,  0.0010...]]))</span>
<div class="newline"></div></pre></div>
</div>
<aside class="topic green">
<p class="topic-title"><strong>Exercise</strong></p>
<p>Do the same with omega = 3. What is the difficulty?</p>
</aside>
</section>
</section>
<section id="optimization-with-constraints">
<h2><a class="toc-backref" href="#id31" role="doc-backlink"><span class="section-number">2.7.7. </span>Optimization with constraints</a><a class="headerlink" href="#optimization-with-constraints" title="Link to this heading">¶</a></h2>
<section id="box-bounds">
<h3><a class="toc-backref" href="#id32" role="doc-backlink"><span class="section-number">2.7.7.1. </span>Box bounds</a><a class="headerlink" href="#box-bounds" title="Link to this heading">¶</a></h3>
<p>Box bounds correspond to limiting each of the individual parameters of
the optimization. Note that some problems that are not originally written
as box bounds can be rewritten as such via change of variables. Both
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize_scalar()</span></code></a> and <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize()</span></code></a>
support bound constraints with the parameter <code class="docutils literal notranslate"><span class="pre">bounds</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>   <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">((</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)))</span>
<div class="newline"></div><span class="go">  message: CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_&lt;=_PGTOL</span>
<div class="newline"></div><span class="go">  success: True</span>
<div class="newline"></div><span class="go">   status: 0</span>
<div class="newline"></div><span class="go">      fun: 1.5811388300841898</span>
<div class="newline"></div><span class="go">        x: [ 1.500e+00  1.500e+00]</span>
<div class="newline"></div><span class="go">      nit: 2</span>
<div class="newline"></div><span class="go">      jac: [-9.487e-01 -3.162e-01]</span>
<div class="newline"></div><span class="go">     nfev: 9</span>
<div class="newline"></div><span class="go">     njev: 3</span>
<div class="newline"></div><span class="go">     hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;</span>
<div class="newline"></div></pre></div>
</div>
<a class="reference external image-reference" href="auto_examples/plot_constraints.html"><img alt="../../_images/sphx_glr_plot_constraints_002.png" class="align-right" src="../../_images/sphx_glr_plot_constraints_002.png" style="width: 225.0px; height: 187.5px;" /></a>
</section>
<section id="general-constraints">
<h3><a class="toc-backref" href="#id33" role="doc-backlink"><span class="section-number">2.7.7.2. </span>General constraints</a><a class="headerlink" href="#general-constraints" title="Link to this heading">¶</a></h3>
<p>Equality and inequality constraints specified as functions: <img class="math" src="../../_images/math/1601b03f200709bde02c1c3f58fa64a41bf102a9.png" alt="f(x) = 0"/>
and <img class="math" src="../../_images/math/9b02854b56ae41e2bbb0eb95247040bd026a86bb.png" alt="g(x) &lt; 0"/>.</p>
<ul>
<li><p><a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_slsqp.html#scipy.optimize.fmin_slsqp" title="(in SciPy v1.12.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.fmin_slsqp()</span></code></a> Sequential least square programming:
equality and inequality constraints:</p>
<a class="reference external image-reference" href="auto_examples/plot_non_bounds_constraints.html"><img alt="../../_images/sphx_glr_plot_non_bounds_constraints_001.png" class="align-right" src="../../_images/sphx_glr_plot_non_bounds_constraints_001.png" style="width: 225.0px; height: 187.5px;" /></a>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">3</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">constraint</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<div class="newline"></div><span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
<div class="newline"></div>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<div class="newline"></div><span class="gp">&gt;&gt;&gt; </span><span class="n">sp</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;fun&quot;</span><span class="p">:</span> <span class="n">constraint</span><span class="p">,</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;ineq&quot;</span><span class="p">})</span>
<div class="newline"></div><span class="go"> message: Optimization terminated successfully</span>
<div class="newline"></div><span class="go"> success: True</span>
<div class="newline"></div><span class="go">  status: 0</span>
<div class="newline"></div><span class="go">     fun: 2.4748737350439685</span>
<div class="newline"></div><span class="go">       x: [ 1.250e+00  2.500e-01]</span>
<div class="newline"></div><span class="go">     nit: 5</span>
<div class="newline"></div><span class="go">     jac: [-7.071e-01 -7.071e-01]</span>
<div class="newline"></div><span class="go">    nfev: 15</span>
<div class="newline"></div><span class="go">    njev: 5</span>
<div class="newline"></div></pre></div>
</div>
</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>The above problem is known as the <a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Lasso</a>
problem in statistics, and there exist very efficient solvers for it
(for instance in <a class="reference external" href="https://scikit-learn.org">scikit-learn</a>). In
general do not use generic solvers when specific ones exist.</p>
</div>
<aside class="topic">
<p class="topic-title"><strong>Lagrange multipliers</strong></p>
<p>If you are ready to do a bit of math, many constrained optimization
problems can be converted to non-constrained optimization problems
using a mathematical trick known as <a class="reference external" href="https://en.wikipedia.org/wiki/Lagrange_multiplier">Lagrange multipliers</a>.</p>
</aside>
</section>
</section>
<section id="id2">
<h2><a class="toc-backref" href="#id34" role="doc-backlink"><span class="section-number">2.7.8. </span>Full code examples</a><a class="headerlink" href="#id2" title="Link to this heading">¶</a></h2>
</section>
<section id="id3">
<h2><a class="toc-backref" href="#id35" role="doc-backlink"><span class="section-number">2.7.9. </span>Examples for the mathematical optimization chapter</a><a class="headerlink" href="#id3" title="Link to this heading">¶</a></h2>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="Draws a figure explaining noisy vs non-noisy optimization"><img alt="" src="../../_images/sphx_glr_plot_noisy_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_noisy.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-noisy-py"><span class="std std-ref">Noisy optimization problem</span></a></p>
  <div class="sphx-glr-thumbnail-title">Noisy optimization problem</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Draws a figure to explain smooth versus non smooth optimization."><img alt="" src="../../_images/sphx_glr_plot_smooth_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_smooth.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-smooth-py"><span class="std std-ref">Smooth vs non-smooth</span></a></p>
  <div class="sphx-glr-thumbnail-title">Smooth vs non-smooth</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A curve fitting example"><img alt="" src="../../_images/sphx_glr_plot_curve_fitting_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_curve_fitting.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-curve-fitting-py"><span class="std std-ref">Curve fitting</span></a></p>
  <div class="sphx-glr-thumbnail-title">Curve fitting</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A figure showing the definition of a convex function"><img alt="" src="../../_images/sphx_glr_plot_convex_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_convex.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-convex-py"><span class="std std-ref">Convex function</span></a></p>
  <div class="sphx-glr-thumbnail-title">Convex function</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An exercise of finding minimum. This exercise is hard because the function is very flat around ..."><img alt="" src="../../_images/sphx_glr_plot_exercise_flat_minimum_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_exercise_flat_minimum.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-exercise-flat-minimum-py"><span class="std std-ref">Finding a minimum in a flat neighborhood</span></a></p>
  <div class="sphx-glr-thumbnail-title">Finding a minimum in a flat neighborhood</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example showing how to do optimization with general constraints using SLSQP and cobyla."><img alt="" src="../../_images/sphx_glr_plot_non_bounds_constraints_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_non_bounds_constraints.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-non-bounds-constraints-py"><span class="std std-ref">Optimization with constraints</span></a></p>
  <div class="sphx-glr-thumbnail-title">Optimization with constraints</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Illustration of 1D optimization: Brent&#x27;s method"><img alt="" src="../../_images/sphx_glr_plot_1d_optim_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_1d_optim.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-1d-optim-py"><span class="std std-ref">Brent’s method</span></a></p>
  <div class="sphx-glr-thumbnail-title">Brent's method</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="A small figure explaining optimization with constraints"><img alt="" src="../../_images/sphx_glr_plot_constraints_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_constraints.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-constraints-py"><span class="std std-ref">Constraint optimization: visualizing the geometry</span></a></p>
  <div class="sphx-glr-thumbnail-title">Constraint optimization: visualizing the geometry</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="Plots the results from the comparison of optimizers."><img alt="" src="../../_images/sphx_glr_plot_compare_optimizers_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_compare_optimizers.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-compare-optimizers-py"><span class="std std-ref">Plotting the comparison of optimizers</span></a></p>
  <div class="sphx-glr-thumbnail-title">Plotting the comparison of optimizers</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="The challenge here is that Hessian of the problem is a very ill-conditioned matrix. This can ea..."><img alt="" src="../../_images/sphx_glr_plot_exercise_ill_conditioned_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_exercise_ill_conditioned.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-exercise-ill-conditioned-py"><span class="std std-ref">Alternating optimization</span></a></p>
  <div class="sphx-glr-thumbnail-title">Alternating optimization</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="An example demoing gradient descent by creating figures that trace the evolution of the optimiz..."><img alt="" src="../../_images/sphx_glr_plot_gradient_descent_thumb.png" />
<p><a class="reference internal" href="auto_examples/plot_gradient_descent.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-gradient-descent-py"><span class="std std-ref">Gradient descent</span></a></p>
  <div class="sphx-glr-thumbnail-title">Gradient descent</div>
</div></div><div class="toctree-wrapper compound">
</div>
<div class="sphx-glr-footer sphx-glr-footer-gallery docutils container">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/2250da8939d81b5cc457b85593005b83/auto_examples_python.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">auto_examples_python.zip</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/b4f7dfe5dea520e69bca7a7b06d7ba42/auto_examples_jupyter.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">all</span> <span class="pre">examples</span> <span class="pre">in</span> <span class="pre">Jupyter</span> <span class="pre">notebooks:</span> <span class="pre">auto_examples_jupyter.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><strong>Other Software</strong></p>
<p>SciPy tries to include the best well-established, general-use,
and permissively-licensed optimization algorithms available. However,
even better options for a given task may be available in other libraries;
please also see <a class="reference external" href="https://github.com/xuy/pyipopt">IPOPT</a> and <a class="reference external" href="https://esa.github.io/pygmo2/">PyGMO</a>.</p>
</div>
<p><div style="clear: both"></div></p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">2.7. Mathematical optimization: finding minima of functions</a><ul>
<li><a class="reference internal" href="#knowing-your-problem">2.7.1. Knowing your problem</a><ul>
<li><a class="reference internal" href="#convex-versus-non-convex-optimization">2.7.1.1. Convex versus non-convex optimization</a></li>
<li><a class="reference internal" href="#smooth-and-non-smooth-problems">2.7.1.2. Smooth and non-smooth problems</a></li>
<li><a class="reference internal" href="#noisy-versus-exact-cost-functions">2.7.1.3. Noisy versus exact cost functions</a></li>
<li><a class="reference internal" href="#constraints">2.7.1.4. Constraints</a></li>
</ul>
</li>
<li><a class="reference internal" href="#a-review-of-the-different-optimizers">2.7.2. A review of the different optimizers</a><ul>
<li><a class="reference internal" href="#getting-started-1d-optimization">2.7.2.1. Getting started: 1D optimization</a></li>
<li><a class="reference internal" href="#gradient-based-methods">2.7.2.2. Gradient based methods</a><ul>
<li><a class="reference internal" href="#some-intuitions-about-gradient-descent">Some intuitions about gradient descent</a></li>
<li><a class="reference internal" href="#conjugate-gradient-descent">Conjugate gradient descent</a></li>
</ul>
</li>
<li><a class="reference internal" href="#newton-and-quasi-newton-methods">2.7.2.3. Newton and quasi-newton methods</a><ul>
<li><a class="reference internal" href="#newton-methods-using-the-hessian-2nd-differential">Newton methods: using the Hessian (2nd differential)</a></li>
<li><a class="reference internal" href="#quasi-newton-methods-approximating-the-hessian-on-the-fly">Quasi-Newton methods: approximating the Hessian on the fly</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#full-code-examples">2.7.3. Full code examples</a></li>
<li><a class="reference internal" href="#examples-for-the-mathematical-optimization-chapter">2.7.4. Examples for the mathematical optimization chapter</a><ul>
<li><a class="reference internal" href="#gradient-less-methods">2.7.4.12. Gradient-less methods</a><ul>
<li><a class="reference internal" href="#a-shooting-method-the-powell-algorithm">A shooting method: the Powell algorithm</a></li>
<li><a class="reference internal" href="#simplex-method-the-nelder-mead">Simplex method: the Nelder-Mead</a></li>
</ul>
</li>
<li><a class="reference internal" href="#global-optimizers">2.7.4.13. Global optimizers</a><ul>
<li><a class="reference internal" href="#brute-force-a-grid-search">Brute force: a grid search</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#practical-guide-to-optimization-with-scipy">2.7.5. Practical guide to optimization with SciPy</a><ul>
<li><a class="reference internal" href="#choosing-a-method">2.7.5.1. Choosing a method</a></li>
<li><a class="reference internal" href="#making-your-optimizer-faster">2.7.5.2. Making your optimizer faster</a></li>
<li><a class="reference internal" href="#computing-gradients">2.7.5.3. Computing gradients</a></li>
<li><a class="reference internal" href="#synthetic-exercices">2.7.5.4. Synthetic exercices</a></li>
</ul>
</li>
<li><a class="reference internal" href="#special-case-non-linear-least-squares">2.7.6. Special case: non-linear least-squares</a><ul>
<li><a class="reference internal" href="#minimizing-the-norm-of-a-vector-function">2.7.6.1. Minimizing the norm of a vector function</a></li>
<li><a class="reference internal" href="#curve-fitting">2.7.6.2. Curve fitting</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optimization-with-constraints">2.7.7. Optimization with constraints</a><ul>
<li><a class="reference internal" href="#box-bounds">2.7.7.1. Box bounds</a></li>
<li><a class="reference internal" href="#general-constraints">2.7.7.2. General constraints</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id2">2.7.8. Full code examples</a></li>
<li><a class="reference internal" href="#id3">2.7.9. Examples for the mathematical optimization chapter</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="../image_processing/auto_examples/plot_spectral_clustering.html"
                          title="previous chapter"><span class="section-number">2.6.8.24. </span>Segmentation with spectral clustering</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="auto_examples/plot_noisy.html"
                          title="next chapter"><span class="section-number">2.7.4.1. </span>Noisy optimization problem</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/advanced/mathematical_optimization/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="auto_examples/plot_noisy.html" title="2.7.4.1. Noisy optimization problem"
             >next</a></li>
        <li class="right" >
          <a href="../image_processing/auto_examples/plot_spectral_clustering.html" title="2.6.8.24. Segmentation with spectral clustering"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">Scientific Python Lectures</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" ><span class="section-number">2. </span>Advanced topics</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">2.7. </span>Mathematical optimization: finding minima of functions</a></li>
     
    <!-- Insert a tip toggle in the navigation bar -->
        <li class="left">
        <!-- On click, expand all tips -->
        <!--
        <a>
            <img src="../../_static/plus.png"
                 alt="Expand tips" style="padding: 1ex;"/>
            <span class="hiddenlink">Collapse document to compact view</span>
        </a>
        -->
        </li>
    <li class="right edit_on_github"><a href="https://github.com/scipy-lectures/scientific-python-lectures/edit/main/advanced/mathematical_optimization/index.rst">Edit
      <span class="tooltip">
        Improve this page:<br/>Edit it on Github.
      </span>
    </a>
    </li>
    
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2024.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.2.6.
    </div>
  </body>
</html>